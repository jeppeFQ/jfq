[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Lorem ipsum odor amet, consectetuer adipiscing elit. Quam nullam pretium malesuada potenti commodo rutrum molestie tincidunt sodales. Risus nulla dui faucibus odio est phasellus tempus. Sollicitudin dapibus nunc ex congue nostra sapien velit. Praesent pellentesque vitae sociosqu orci magnis habitant maximus metus quisque. Velit tempus ad sodales hac; suspendisse suscipit.\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem. Himenaeos adipiscing ultrices ex; rutrum dignissim turpis. Libero sociosqu lacinia nibh; potenti elit viverra. Senectus fames fringilla praesent nascetur lacus lobortis dui tortor. Felis ridiculus lorem senectus convallis conubia nec phasellus nisi. Potenti sodales rhoncus et penatibus auctor morbi erat augue iaculis. Egestas dictumst phasellus class nec facilisis sapien lectus maximus iaculis. Quam nascetur fusce vivamus proin dolor; magna tempus curae.\n\n\nHi! I’m a side note!\n\nDIREK\n\nDIREK\n\n2+2 \n\n[1] 4\n\n\n\n\n\nMNcontact\n\nMNcontact\nLorem ipsum odor amet, consectetuer adipiscing elit. Quam nullam pretium malesuada potenti commodo rutrum molestie tincidunt sodales. Risus nulla dui faucibus odio est phasellus tempus. Sollicitudin dapibus nunc ex congue nostra sapien velit. Praesent pellentesque vitae sociosqu orci magnis habitant maximus metus quisque. Velit tempus ad sodales hac; suspendisse suscipit.\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem. Himenaeos adipiscing ultrices ex; rutrum dignissim turpis. Libero sociosqu lacinia nibh; potenti elit viverra. Senectus fames fringilla praesent nascetur lacus lobortis dui tortor. Felis ridiculus lorem senectus convallis conubia nec phasellus nisi. Potenti sodales rhoncus et penatibus auctor morbi erat augue iaculis. Egestas dictumst phasellus class nec facilisis sapien lectus maximus iaculis. Quam nascetur fusce vivamus proin dolor; magna tempus curae.\nFames vivamus ante fames velit sapien, bibendum magna habitasse laoreet. Lobortis eros metus dignissim taciti conubia ex. Semper mattis nam maecenas aliquam torquent tempus mus. Finibus diam natoque nam neque rhoncus suscipit. Imperdiet senectus porta cursus dui fames taciti vel? Est lorem nibh ut sem, bibendum vel malesuada orci. At sed rhoncus penatibus ultrices suspendisse.\nUt inceptos natoque lacus risus massa facilisis. Pretium imperdiet dictum fusce vehicula ac magnis sollicitudin suscipit vel. Ornare semper aliquam mollis tristique integer maximus eros. Phasellus ac fusce potenti sit eros in nulla duis vulputate. Vivamus nostra est mi consequat varius laoreet accumsan morbi. Lectus faucibus facilisi nunc pulvinar et faucibus ultricies cursus.\nMassa efficitur consequat augue non rutrum ut lobortis adipiscing nulla. Ornare euismod mi varius; mauris phasellus bibendum mus metus sagittis. Facilisi arcu a congue nascetur quam varius morbi. Luctus dictumst sapien rhoncus commodo facilisis maecenas ad. Efficitur sapien nibh ullamcorper at semper ridiculus suspendisse. Id aptent nullam semper leo magnis. Blandit diam hac pellentesque ex litora sagittis convallis ornare convallis. Semper convallis volutpat auctor taciti mi habitasse finibus nunc.\n\n\nMassa efficitur\nInteger habitant varius hendrerit torquent sodales. Fusce consectetur consectetur bibendum mattis justo aenean mauris nam sed. Potenti vel a; cubilia turpis porta faucibus donec. Luctus tempor feugiat elit ligula mollis. Efficitur risus consectetur tellus cras in scelerisque. Tincidunt faucibus convallis neque elit, potenti purus ac convallis. Luctus pharetra condimentum per vitae blandit eleifend ac condimentum. Arcu arcu ut ullamcorper class semper.\nHac magna eros fermentum vitae aliquet nisi mauris. Tortor id diam tortor magna nunc est nisi feugiat hac. Sit nunc cras duis primis sollicitudin aenean penatibus commodo. Porttitor netus facilisi per class lorem nibh. Finibus ultricies penatibus; mauris justo feugiat himenaeos sapien maximus. Facilisis habitasse nibh eu augue nisi tincidunt elit suspendisse. Nam molestie netus ex nullam in. Feugiat magna blandit quisque feugiat curabitur himenaeos convallis accumsan. Turpis aliquam quis dictumst quam; ultricies commodo consectetur."
  },
  {
    "objectID": "projects/mncontact.html",
    "href": "projects/mncontact.html",
    "title": "",
    "section": "",
    "text": "MNcontact\nLorem ipsum odor amet, consectetuer adipiscing elit. Quam nullam pretium malesuada potenti commodo rutrum molestie tincidunt sodales. Risus nulla dui faucibus odio est phasellus tempus. Sollicitudin dapibus nunc ex congue nostra sapien velit. Praesent pellentesque vitae sociosqu orci magnis habitant maximus metus quisque. Velit tempus ad sodales hac; suspendisse suscipit.\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem. Himenaeos adipiscing ultrices ex; rutrum dignissim turpis. Libero sociosqu lacinia nibh; potenti elit viverra. Senectus fames fringilla praesent nascetur lacus lobortis dui tortor. Felis ridiculus lorem senectus convallis conubia nec phasellus nisi. Potenti sodales rhoncus et penatibus auctor morbi erat augue iaculis. Egestas dictumst phasellus class nec facilisis sapien lectus maximus iaculis. Quam nascetur fusce vivamus proin dolor; magna tempus curae.\nFames vivamus ante fames velit sapien, bibendum magna habitasse laoreet. Lobortis eros metus dignissim taciti conubia ex. Semper mattis nam maecenas aliquam torquent tempus mus. Finibus diam natoque nam neque rhoncus suscipit. Imperdiet senectus porta cursus dui fames taciti vel? Est lorem nibh ut sem, bibendum vel malesuada orci. At sed rhoncus penatibus ultrices suspendisse.\nUt inceptos natoque lacus risus massa facilisis. Pretium imperdiet dictum fusce vehicula ac magnis sollicitudin suscipit vel. Ornare semper aliquam mollis tristique integer maximus eros. Phasellus ac fusce potenti sit eros in nulla duis vulputate. Vivamus nostra est mi consequat varius laoreet accumsan morbi. Lectus faucibus facilisi nunc pulvinar et faucibus ultricies cursus.\nMassa efficitur consequat augue non rutrum ut lobortis adipiscing nulla. Ornare euismod mi varius; mauris phasellus bibendum mus metus sagittis. Facilisi arcu a congue nascetur quam varius morbi. Luctus dictumst sapien rhoncus commodo facilisis maecenas ad. Efficitur sapien nibh ullamcorper at semper ridiculus suspendisse. Id aptent nullam semper leo magnis. Blandit diam hac pellentesque ex litora sagittis convallis ornare convallis. Semper convallis volutpat auctor taciti mi habitasse finibus nunc.\n\n\nMassa efficitur\nInteger habitant varius hendrerit torquent sodales. Fusce consectetur consectetur bibendum mattis justo aenean mauris nam sed. Potenti vel a; cubilia turpis porta faucibus donec. Luctus tempor feugiat elit ligula mollis. Efficitur risus consectetur tellus cras in scelerisque. Tincidunt faucibus convallis neque elit, potenti purus ac convallis. Luctus pharetra condimentum per vitae blandit eleifend ac condimentum. Arcu arcu ut ullamcorper class semper.\nHac magna eros fermentum vitae aliquet nisi mauris. Tortor id diam tortor magna nunc est nisi feugiat hac. Sit nunc cras duis primis sollicitudin aenean penatibus commodo. Porttitor netus facilisi per class lorem nibh. Finibus ultricies penatibus; mauris justo feugiat himenaeos sapien maximus. Facilisis habitasse nibh eu augue nisi tincidunt elit suspendisse. Nam molestie netus ex nullam in. Feugiat magna blandit quisque feugiat curabitur himenaeos convallis accumsan. Turpis aliquam quis dictumst quam; ultricies commodo consectetur."
  },
  {
    "objectID": "workshop/plaintext.html",
    "href": "workshop/plaintext.html",
    "title": "",
    "section": "",
    "text": "Plain text"
  },
  {
    "objectID": "workshop/regression.html",
    "href": "workshop/regression.html",
    "title": "",
    "section": "",
    "text": "Regression"
  },
  {
    "objectID": "workshop/nb.html",
    "href": "workshop/nb.html",
    "title": "",
    "section": "",
    "text": "Case …\n\n\nSuperviseret Machine Learning (SML)\nI dag er fokus kun på superviseret ML, da vi kun har en enkelt workshop i dag og der vil være for mange statistiske forudsætninger til de to andre hovedtyper.\nSML fungerer ved, at vi giver modellen data, hvor vi kender det rigtige svar. Det kunne være, om en besked er spam eller ej, om et produkt er populær baseret på salgsdata, eller hvilken temperatur der vil være i morgen baseret på historiske målinger.\n\nI besked-eksemplet vil det altså sige at vi har et datasæt bestående af SMSer, hvor hver SMS i den data vi træner vores model på er kodet, dvs. tilskrevet et label, der indikerer om SMSen er spam (label=1) eller ikke-spam – “ham” – (label=0).\n\nModellen lærer sammenhænge mellem de inputdata (features, ord), som vi fodrer den med, og de kendte svar (labels, spam/ham). Når modellen er trænet, og den er vurderet til at være god nok, kan vi bruge den til at forudsige labels for nye data, hvor vi ikke kender svaret på forhånd.\nDet største problem i at arbejde med tekst i ML er at ML-algoritme ikke kan arbejde direkte med tekst. De kræver numeriske inputs for at kunne udføre matematiske operationer (se grundbog).\nVi er altså nødt til at konvertere tekst til en numerisk repræsentation. I denne kontekst kaldes denne proces for vectorisering.\nI denne proces transformerer og repræsenterer vi hvert tekstdokument (fx en SMS) som en række tal eller en vektor.\n\n\nKlassifikationsalgoritme\nNaive Bayes er en algoritme til at løse et konkret klassifikationsproblem relateret til naturligt sprog\nNaive Bayes er en probabilistisk klassifikationsmodel, baseret på Bayes’ teorem. Algoritmen fungerer ved at beregne sandsynligheden for, at en besked tilhører en bestemt klasse, givet dens indhold (dvs. de ord, der optræder i beskeden).\nModellen kaldes for “Naiv” grundet en central antagelse om at alle features (i vores tilfælde ord) er uafhængige af hinanden. Denne antagelse er ikke realistisk, da ord normalt ikke optræder helt uafhængigt af hinanden (ord i sætninger er ofte afhængige af hinanden). Det gør algoritmen enkel og hurtig, og den fungerer alligevel godt i praksis, som er blevet illustreret i tekniske detaljer mange steder.\nModellens formål er at lære forholdet mellem de inputdata, vi giver den (features), og de kendte labels, så den kan forudsige labels for nye, ukendte data.\nDet vil altså sige at vi har med et klassifikationsproblem at gøre. Modellen skal forudsige, hvilken kategori noget tilhører, og virke som et spam-filter, hvor vi klassificerer beskeder som enten “spam” eller “ikke-spam”, og i en praktisk applikation kan sende indkomne beskeder ind i forskellige mapper, som I kender fra jeres e-mail.\n\nBayes’ Teorem\nBayes’ Theorem handler om at beregne betingede sandsynligheder, der giver os en måde at opdatere vores viden baseret på nye data. Det har givet navn til en hel gren i statistikke, bayesisk statistik, som står i kontrast til frekvensstatistik (som er det i med al sandsynlighed kender fra gymnasiet og det i skal lære på 4. semester).\nBayes’ teorem er givet ved:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\nHvor,\n\nP(A∣B): Sandsynligheden for A, givet B. Dette kalder vi den posterior sandsynligheden.\nP(B∣A): Sandsynligheden for B, givet A. Dette kalder vi likelihood (betinget sandsynlighed).\nP(A): Sandsynligheden for A uden at tage hensyn til B. Dette kalder vi prior sandsynligheden.\nP(B): Sandsynligheden for B, uanset hvad A er.\n\nI kontekst og i en klassifikationssammenhæng er:\n\nA klassen (spam eller ham), og\nB er de observerede data (de ord, der optræder i beskeden).\n\nog det vi er interesserde i er at bestemme P(spam∣ord): sandsynligheden for, at en besked er spam, givet at visse ord optræder.\nRepitation af formål og hvad vi vil implementere i Python er:\n\nSandsynligheden for at en SMS er spam, baseret på fremkomsten/tilstedeværelsen af et givent ord, er proportionelt til sandsynligheden for at ordet fremkommer i spam-SMSer og den a priori sandsynlighed for at en tilfældig SMS er spam.\n\n\nP(\\text{spam}|ord)\\propto P(ord|\\text{spam}) P(\\text{spam})\n\n\nSpg.: Hvordan implimenterer vi denne model i Python på en måde, der kan “lære” maskinen at genkende spam-SMSer?\n\n\n\nAnvendelse af algoritmen til tekstklassificering\nVi bruger Naive Bayes til tekstklassifikation for at forudsige om en besked er spam eller ham baseret på sandsynligheden for de enkelte ord, der optræder i beskeden, tilhører en given klasse.\nDen generelle Naive Bayes-klassifikator for to klasser (spam eller ikke-spam) er formuleret som:\n \\hat{y} = \\underset{c}{\\operatorname{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i | c) \nHvor,\n\n\\hat{y} er den forudsagte klasse.\nc er en af klasserne (spam eller ikke-spam).\nP(c) er prior sandsynligheden for klassen c (sandsynligheden for, at en tilfældig besked er spam).\nP(x_{i}∣c) er sandsynligheden for ordet x_{i}, givet klassen c.\nn er antallet af ord i beskeden.\n\nMed formlen beregner vi sandsynligheden for, at en besked tilhører hver klasse (spam eller ikke-spam), og vælger den klasse, der har den højeste sandsynlighed: Vi vælger den klasse (c), hvor P(c|x) er størst, indikeret i formlen med \\underset{c}{\\operatorname{argmax}}\nFremgangsmåde:\n\nPrior sandsynlighed, P(c), beregnes ved at tælle, hvor mange af vores træningsbeskeder, der er spam i forhold til det samlede antal beskeder: P(spam)= \\frac{\\text{Antal spam-beskeder}}{\\text{Totalt antal beskeder}}\nLikelihood (betinget sandsynlighed), P(x_{i}∣c) beregnes som: P(x_{i}∣spam)= \\frac{\\text{Antal spam-beskeder, der indeholder } x_{i}}{\\text{Antal spam-beskeder totalt}}\n\nHvad vi er udregner, er hvor ofte hvert unikke ord i vores SAMLEDE TEKSTMATERIALE optræder i spam-beskeder (eller ikke-spam-beskeder), og produktet af sandsynlighederne for hvert enkelt ord i en given tekst, definere om teksten sandsynligvis er spam (eller ikke-spam)\nSelvom algoritmen er “naiv” og antagelsen om at alle ord er uafhængige, i praksis som udgangspunkt ikke holder, bestemmer vi stadig sandsynligheden for om en besked er spam eller ham som produktet af sandsynlighederne for de enkelte ord. Det er mange gange vist at denne “fejlantagelse” ikke er et problem i større mængder tekstdata.\n\nVi bruger træningsdata til at beregne P(spam) og P(ham)\nVi beregner sandsynlighederne for ordene “Congratulations”, “won”, “free”, osv. under begge klasser (spam og ham). Altså, hvad er sandsynligheden for at “free” (x_{free}) tilhører hhv. spam og ham klassen, givet fremkomsten af x_{free} i SMSer klassificeret som spam eller ham.\nVi multiplicerer sandsynlighederne for de enkelte ord – $x_{Congratulation} + x_{won} + x_{free} + $ – og vælger den klasse med den højeste sandsynlighed. Altså, hvert ord i en SMS har en sandsynlighed for at tilhører spam eller ham, og givet disse enkelte ord, hvor sandsynligt er det så for at SMS i sin helhed er spam eller ham.\n\nAltså, hvis ord som “won” og “free” ofte forekommer i spam-beskeder, vil Naive Bayes tildele beskeder med (store) fremkomster af disse ord en høj sandsynlighed for at være spam, og med denne sandsynlighedsargumentation klassificere SMSen som spam.\n\n\nEn lille, men central, sidebemærkning…\nDer kan opstå en problematisk udfordring, hvis et ord i udenfor vores træningsdata ikke fremgår i træningsdataen, da det vil “nulstille” den samlede sandsynlighed når vi multiplicerer.\nDette overkommes ved at inkludere en metode, der kaldes Laplace-smoothing, hvor alle betingede sandsynligheder tilføjes en lille konstant (værdi), således at ingen sandsynligheder er 0:\nP(x_{i}∣c)= \\frac{\\text{Totalt antal ord i klassen }+V}{\\text{Antal gange ordet optræder i klassen} +1}\nHvor, V er størrelsen af ordforrådet i vores corpus (antallet af unikke ord i træningsdataene). Med dette undgår vi nul-sandsynligheder.\n\n\nModel-træning\nI arbejdet med superviseret Machine Learning arbejder vi med vores data som opdelt i hhv. trænings- og testdata. Den data vi arbejder med, er et datasæt som vi har kvalitativt kodet med de korrekte labels ud fra vores forhåndsviden. Med denne opdeling er det muligt både at træne vores model og evaluere vores model, for at kunne vurdere hvordan modellen performer på nye, usete data.\n\n\nTræningsdata\nTræningsdataen er det datasæt, som vi træner vores model på. Datasættet indeholder både features (ord) og labels (korrekte kategorier). Det vil sige, vi ved altså hvad den rigtige kategori til vores tekster er, for at vores model at udregne det mønster, der kendetegner hver kategori.\nMed andre ord, når vi træner en Naive Bayes-model, “lærer” den at forstå sammenhængen mellem de input og de tilknyttede labels.\nEn klassisk opdeling er, at træningsdataen udgør 80% af den kvalitativt kodet data.\n\n\nTestdata\nTestdata udgør den anden del af den kvalitativt kodede data (her 20%). Testdataene bruges til at evaluere modelens præstation og generaliseringsevne og formålet med testdata er at give et mål for, hvordan modellen vil præstere på nye, usete data. Det vil altså sige at modellen ikke har “set” denne data (og er grunden til at vi skal have Laplace Smoothing…)\n\n\n\nEksempel …\n\nKlargøring af tekstdata\nDer er flere måde, hvorpå vi kan vektoriserer tekster, men centrale of typiske i denne form for analyse er:\n\nBag of Words (BoW)\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\n\nBoW\nBag of Words er den mest simpel metode til at transformere tekst til numerisk form. Det fungerer ved at tælle, hvor mange gange hvert ord forekommer i et dokument, uden at tage højde for ordets rækkefølge eller kontekst. Resultatet er en vektor, der repræsenterer frekvensen af hvert ord i dokumentet. Eksempel på BoW:\nTekst 1: Jeg elsker spam\nTekst 2: Jeg kan ikke fordrage spam\nFørst opretter vi et ordforråd (vocabulary) baseret på alle de unikke ord i vores dokumenter:\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\nvocab = ['Jeg', 'elsker', 'spam', 'kan', 'ikke', 'fordrage']\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\nffffffffffff\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\ntekst1 = [1, 1, 1, 0, 0, 0]\ntekst2 = [1, 0, 1, 1, 1, 1]\ndf = pd.DataFrame([tekst1, tekst2], columns=vocab, index=['tekst1', 'tekst2'])\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\nffffffffffff\n\n\nTF-IDF\nTF-IDF tager, i modsætning til BoW, højde for, hvor ofte et ord forekommer i en tekst, i forhold til hvor ofte det forekommer i hele datasættet. Dette hjælper med at nedvægte meget almindelige ord (såsom “is”, “am,”the”, osv.), som sandsynligvis ikke bidrager meget til meningen af dokumentet, og fremhæve ord, der er særligt vigtige for den specifikke besked (såsom “free”, “won”).\nTF-IDF for et ord x i et dokument d er givet ved:\n\\text{TF-IDF}(x,d)=\\text{TF}(x,d) \\times \\text{IDF}(x)\nHvor:\n\nTF (Term Frequency): Måler hvor ofte ordet x forekommer i dokumentet d.\nIDF (Inverse Document Frequency): log ⁡\\left( \\frac{N}{df(x)} \\right ), hvor N er det totale antal dokumenter, og df(x) er antallet af dokumenter, som indeholder x.\n\nHermed sikrer vi at vi ikke vægter almindelige ord for højt, men i stedet fokuserer på de vigtigere ord.\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\ncorpus = [\"Jeg elsker spam\",\"Jeg kan ikke fordrage spam\"]\n\ndf = pd.DataFrame({'dokument': corpus})\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\nffffffffffff\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn TF for hvert ord i dokumentet:\n\n# Tokenisere dokumenter:\n# hvad kalder vi det når vi skriver .apply(lambda x: ...)?\n# og hvad sker der?\ndf['tokens'] = df['dokument'].apply(lambda x: x.split())\n\n# Beregn antallet af ord i hvert dokument\n# Hvad sker der her?\ndf['total_ord'] = df['tokens'].apply(len)\n\n# En liste af alle tokens:\n# 1. Vi looper først over hver sublist i df['tokens'], der er alle ord i en tekst. Dvs. vi looper over hver række i kolonnen 'tokens'.\n# 2. Når vi har en specifik sublist, \"liste_med_ord\", looper vi nu over hvert enkelt token (ord, x) i denne subliste.\n# 3. For hvert token i hver sublist, føjes dette token til den nye liste alle_tokens med .append().\nalle_tokens = []\nfor liste_med_ord in df['tokens']:\n    for x in liste_med_ord:\n        alle_tokens.append(x)\n\n# Find de unikke tokens:\n# \"set()\" funktion er kun at gemme unikke elementer/værdier\n# \"sorted()\" er med for at organisere vores tokens alfabetisk, men er som sådan ikke nødvendig. Prøv evt. uden.\nunikke_tokens = sorted(set(alle_tokens))\n\n# Udregn TF for hvert dokument for hvert ord\nfor ord in unikke_tokens:\n    df[ord] = df['tokens'].apply(lambda x: x.count(ord) / len(x))\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\nfffffffffff\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn IDF for hvert ord i dokumentet:\n\nimport math # For at få log()-funktionen\n\n# Beregne IDF for hvert ord\n# 1. Definer funktion\ndef bestem_idf(ord, df):\n    # Antal dokumenter der indeholder ordet\n    # 2.: df['tokens'] er en kollonne i vores DataFrame (df)\n    # 3.: .apply(lambda x: ord in x) for hvert dokument (SMS),\n    #     repræsenteret som en liste af ord, tjekker vi om ordet er til stede\n    #     i dokumentet. Funktionen returnerer TRUE eller FALSE (ord in x: True or False?)\n    # 4.: TRUE og FALSE repræsenteres nummerisk som 1 og 0. Ved at summere alle 1ere og 0ere,\n    #     får vi antallet af dokumenter, der indeholder ord x.\n    doks_med_ord = df['tokens'].apply(lambda x: ord in x).sum()\n    # Beregn IDF\n    # 5.: len() giver en værdi for antallet af dokumenter (SMSer). Tælleren i formlen.\n    #     (1 + doks_med_ord) er nævneren i formlen\n    #     .log(...) tager logaritmen.\n    return math.log(len(df) / (1 + doks_med_ord))\n\n# Beregn IDF for hvert unikke ord\n    # 6. Dette kalder vi en \"dictionary comprehension\", fordi koden her går\n    #    gennem alle ord i unikke_tokens og for HVERT ORD i unikke_tokens\n    #    kaldes vores definerede funktion \"bestem_idf\" og tilknytter en IDF-score\n    #    til dette ord.\nidf_scores = {ord: bestem_idf(ord, df) for ord in unikke_tokens}\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\nfffffffffff\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn TF-IDF for hvert ord i hvert dokument\n# 1.: df består af entelte ord (tokens) med en TF værdi (udregnet ovenfor),\n#     hvor unikke_tokens repræsenterer alle unikke ord, som vi looper henover.\n# 2.: Hvert ord har en tilknyttet IDF-værdi, som er udregnet med \"bestem_idf\",\n#     og gemt i \"idf_scores\".\n# Det som dette loop gør er at multiplicere hver enkelt ord TF med IDF, og får\n# dermed TF-IDF for HVERT ORD i vores samlede dokumentdata.\nfor ord in unikke_tokens:\n    df[ord] = df[ord] * idf_scores[ord]\n\n# Print beregnede scores:\n# Hvad sker der her, hvor jeg indenfor [[...]] også anvender en vektor med alle unikke ord?\nprint(df[['dokument'] + unikke_tokens])\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45       \n\n\n\n\n\n\n\nTL;DR\nOpsummeret,\n\nKoden gennemgår hvert unikt ord i dokumenterne.\nVi tæller, hvor mange dokumenter (SMSer) der indeholder det specifikke ord.\nVi beregner IDF for hvert ord baseret på, hvor mange dokumenter det optræder i.\nIDF-værdierne gemmes i en dictionary (idf_scores), hvor hvert ord har en tilknyttet IDF-værdi.\nFor hvert ord multiplicerer vi den tilhørende TF og IDF værdi for at få TF-IDF\n\n\n\n\nTræning\n\n\nTest\n\n\nEvaluering"
  },
  {
    "objectID": "workshop/git.html",
    "href": "workshop/git.html",
    "title": "",
    "section": "",
    "text": ".git and versioncontrol\nFor installation, se …\nVersionkontrol er et system (software), der holder styr på ændringer af filer over tid, der gør det muligt at genskabe vores tidligere arbejde. Virker for (stort set) alle filer.\nPå større projekter—hvor flere er involveret—er det vigtigt at have kontrol over, hvem der foretager ændringer, hvilke ændringer der er blevet foretaget, og hvordan man kan rulle tilbage til tidligere versioner, hvis noget går galt.\nI et langsigtet perspektiv vil Git hjælpe dig med at holde et projekt organiseret, muliggøre (mere) effektivt samarbejde og sikre, at vi altid har en backup af dine fremskridt.\n\n\nGør dette …\n/projektarbejde\n└──/backup\n    ├── projekt_281024.docx\n    ├── projekt_311024.docx\n    ├── projekt_041224.docx\n    ├── projekt_final.docx\n    ├── projekt_final2.docx\n    ├── projekt_final3.docx\n    ├── projekt_final_final.docx\n    └── projekt_FINAL.docx\n\n… til dette\n/projektarbejde\n├── .git\n└── projekt.docx\n\n\n\nGit i praksis\nVersionsstyringsprocessen med afsæt i .git skelletet består af 3 stadier:\n\nWorking Directory: den mappe, hvor vi kørte git init. Alt der ændres her spores af Git, men det gemmes (committes) ikke automatisk . Arbejdsområdet er der hvor .git er gemt og indeholder vores faktiske filer og mapper, som vi ser og redigerer på din computer. Når vi redigerer en fil i vores projekt, bliver ændringen først gjort i arbejdsområdet. Filer, der arbejdes på, får tagget M (modified), som betyder at Git har registeret en ændring, men den er ikke blevet gemt i versionshistorikken endnu.\nStaging Area: De ændringer, som du ønsker registreret i næste commit bliver flyttet til et staging area med git add . (se ④ nedenfor). Det er ikke som sådan et “sted”, men et snarer et “tag” til de filer, som Git skal gemme. Ingen ændringer er blevet gemt endnu. Det tekniske navn er index, og Stating Area er ikke et “sted” på computeren men en fil i .git mappen, der noterer hvad der skal sendes til versionshistorikken i næste git commit (se ⑥ nedenfor) og er et mellemstadie mellem Working Directory og Repository. Se det som et kladdeområde, hvor du forbereder de ændringer, der skal indgå i en commit. Vi sender filer til Staging Area med: git add. Den primære funktion er at holde vores versionshistorik ren og logisk opdelt. Hvilket gør det lettere at spore ændringer og identificere bugs senere. For at se hvad der er modificeret og/eller staged bruger vi: git status (se &#9314 nedenfor).\nRepository: Når vi bruger kommandoen git commit -m \"besked\" gemmes alt staged data i vores Git-repository og alle ændringer siden sidste commit bliver en permanent del af projektets versionshistorik. Vores repository er commit-historikken, hvor hver commit repræsenterer en version af projektet på et bestemt tidspunkt. Når filer er committed er det sikkert gemt i vores lokale database. Vi sender filer til versionshistorikken med: git commit &lt;fil&gt; (se ⑥ nedenfor). Vi tilgår historikken med: git log (se ⑦ nedenfor).\n\n\n\nBranching\nHver commit repræsenterer et punkt i projektets branch, og du kan navigere frem og tilbage i projektets historie efter behov.\nEn branch i Git repræsenterer en uafhængig udviklingslinje. Vi kan lave ændringer i denne branch uden at påvirke andre branches. Vi kan droppe en branch, hvis ideer var dårlig, eller merge den med vores primære branch, hvis det virkede. (Teknisk relaterer alt dette sig til HEAD-pointeren).\n“This makes using Git a joy because we know we can experiment without the danger of severely screwing things up.” (REF)\n\nEt sikkert workflow\n\nIsolering: Hver branch er isoleret fra andre branches, hvilket betyder, at ændringer i én branch ikke påvirker arbejdet i andre branches.\nSamarbejde: Udviklere kan arbejde på separate branches uden at forstyrre hinandens arbejde. Git gør det muligt at flette branches sammen, når arbejdet er færdigt.\nEksperimentering: Branches gør det nemt at eksperimentere med nye ideer uden risiko. Hvis noget går galt, kan du altid slette branch’en og vende tilbage til en stabil version.\n\n\n\nTilgå versionshistorikken og genskab tidligere stadie\ngit log\ngit checkout &lt;commit-id&gt;\n\n\n\n\nKommandoer\n\n1git config\n2git init\n3git status\n4git add\n5git diff\n6git commit\n7git log\n8git clone\n9git push\n10git pull\n11git remote\n\n\n1\n\nIndstilling af konfigurationsindstillinger (fx brugernavn og e-mail).\n\n2\n\nInitialiserer et nyt Git-repository i den aktuelle mappe. I skal være opmærksom på hvilken mappe I befinder jer i, når i kører git init.\n\n3\n\nViser status for ændringer i arbejdsområdet (fx hvilke filer der er ændret og klar til staging).\n\n4\n\nTilføjer filer til staging-området, så de er klar til næste commit.\n\n5\n\nViser forskelle mellem ændringer i filer, enten fra arbejdsområdet eller staging-området.\n\n6\n\nGemmer de ændringer, der er i staging-området, som en ny version i repository.\n\n7\n\nViser en log over commits i repository, ofte med detaljer som forfatter, dato og commit-besked.\n\n8\n\nHenter et eksisterende repository fra en ekstern kilde (fx GitHub) og opretter en lokal kopi.\n\n9\n\nSender lokale commits til et eksternt repository.\n\n10\n\nHenter og integrerer ændringer fra et eksternt repository til den lokale kopi.\n\n11\n\nAdministrerer forbindelser til eksterne repositories.\n\n\n\n\nI ① … ② Kommandoen skaber en ny undermappe (.git) og er “skelettet” for vores repository. Denne mappe indeholder alle Git’s interne data, der bruges til at spore og administrere versionshistorikken for dit projekt. ③ … ④ … ⑤ … ⑥ … ⑦ … ⑧ … ⑨ … ⑩ … ⑪ …\n\nLokalt repository\nDet lokale repository, er det ligger på vores lokalecomputer (.git mappen).\n\n\nFjern repository\nGrundlæggende fungerer et fjernrepositoryet som et centralt lager på en server, som flere udviklere kan samarbejde om. Disser servere er typisk hostet på platforme som GitHub eller GitLab.\nEt fjernrepositoty kan klones (se ⑧ ovenfor) til vores lokale computer, således vi har en lokal kopi af projektet. Herefter kan vi pull’e og push’e ændringer:\n\nPull: Henter ændringer fra fjernrepository’et til dit lokale repository.\nPush: Skubber ændringer fra dit lokale repository til fjernrepository’et.\n\n\n\nDistribueret versionskontrol\nGit er et distribueret versionskontrolsystem, hvilket betyder, at hver udvikler har en fuld kopi af hele repositoryet (inklusive historik og branches) på deres egen computer.\n\n\n\nØvelse"
  },
  {
    "objectID": "workshop/cli-file.html",
    "href": "workshop/cli-file.html",
    "title": "",
    "section": "",
    "text": "Terminalen: interaktion med computeren (og filsystemet)\nTerminalen er det, der giver os adgang til kommandolinjegrænsefladen (CLI). Selvom den har miste meget af sin position blandt den gennemsnitlige computer-bruger—grundet grafiske brugergrænseflader (GUI)—er den fortsat en meget effektiv måde at interagere med computeren. Særligt på Unix-systemer.\n\nShell\nNår vi anvender CLI, bruger vi en shell, der er et program til fortolkning af kommandoer. De to mest almindelige shell-programmer er:\n\nBash (Bourne Again Shell): Standard på mange Linux-distributioner og tidligere på macOS.\nZsh (Z Shell): Standard på macOS fra og med version 10.15 Catalina.\n\n\n\n\nFilorganisering\nReferer til hvordan vores filer (data) og mapper (directories) er struktureret og lagret på vores lagringsenhed (harddisk, SSD, ekstern enhed, …).\nDenne struktur bestemmer hvordan data hentes og gemmes, og gør det muligt for brugeren eller programmer at finde, tilgå og anvende filer.\nEn mappe (directory) er en container, som indeholder filer og andre mapper, og danner grundlaget for en hierakisk struktur (tree-/træstruktur). Opbygningen er med afsæt i en root-mappe (ikke den egentlige root-mappe, men brugerens hjemmemappe), som indeholder undermapper og filer. Herfra indeholder hver undermappe andre undermapper og filer, hvilket danner et træ af mapper og filer, hvis vi zoomer ud. Med andre ord, (træ-)hierakiet giver en logisk og navigérbar organisering på computeren.\n\nroot directory: I Unix-systemer (MacOS, Linux) betegnes den /. I Windows er der en root-mappe i hvert drev, betegnet med bogstavet for drevet, fx C:\\\nUndermapper: Mapper, der findes inde i andre mapper, fx /home/user/documents eller C:\\Users\\Username\\Documents.\n\n\n\nEt filsystem\n\n\n\nUNIX (MacOS, Linux)\n/\n├── bin                  # Vigtige eksekverbare systemfiler\n├── sbin                 # Systemadministrative eksekverbare filer\n├── etc                  # Systemkonfigurationsfiler\n├── home                 # Brugermapper (personlige filer)\n│   └── jeppe            # Brugeren \"jeppe\"'s hjemmemappe\n│       ├── Documents    # jeppes dokumenter\n│       ├── Downloads    # jeppes downloadede filer\n│       ├── Music        # jeppes musikfiler\n│       ├── Pictures     # jeppes billeder\n│       ├── Videos       # jeppes videofiler\n│       └── Projects     # Personlige kodeprojekter og scripts\n│           └── snake_game\n│               ├── main.py # Python-kode til et snake-spil\n│               └── assets  # Grafikfiler til spillet\n├── root                 # Superbrugerens hjemmemappe\n├── usr                  # Bruger- og systemprogrammer\n│   ├── bin              # Programmer installeret til brugere\n│   ├── lib              # Systemets biblioteker\n│   └── local            # Lokalt installerede programmer\n├── var                  # Variable data som logs og mails\n│   ├── log              # Systemets logfiler\n│   └── tmp              # Midlertidige filer\n├── tmp                  # Midlertidige filer (slettes ved genstart)\n├── dev                  # Systemets enheder som harddiske og terminaler\n├── mnt                  # Monteringspunkt for midlertidige enheder\n│   └── usb-drive        # En USB-nøgle monteret her\n└── media                # Monteringspunkt for eksterne enheder\n    └── jeppe-usb        # jeppes eksterne harddisk hvis monteret\n\n\n\nWindows\nC:\\\n├── Program Files            # Programmer installeret for alle brugere\n├── Program Files (x86)      # 32-bit versioner (på 64-bit systemer)\n├── Users                    # Brugermapper (til hver bruger på systemet)\n│   └── jeppe                # Brugeren \"jeppe\"'s hjemmemappe\n│       ├── Documents        # jeppes dokumenter\n│       ├── Downloads        # jeppes downloadede filer\n│       ├── Music            # jeppes musikfiler\n│       ├── Pictures         # jeppes billeder\n│       ├── Videos           # jeppes videofiler\n│       ├── Desktop          # Filer og genveje på jeppes skrivebord\n│       ├── AppData          # jeppes personlige app-data og indstillinger\n│       └── Projects         # Personlige kodeprojekter og scripts\n│           └── snake_game\n│               ├── main.py  # Python-kode til et snake-spil\n│               └── assets   # Grafikfiler til spillet\n├── Windows                  # Operativsystemets filer\n│   ├── System32             # Vigtige systemfiler \n│   └── Temp                 # Midlertidige filer, der bruges af systemet\n├── ProgramData              # Data, der deles af applikationer på systemet\n└── Temp                     # Midlertidige filer\n\n\n\n\nNavigation: Absolutte og relative stier\n\nAbsolut sti: En sti, der beskriver placeringen af en fil eller mappe i forhold til root-mappen. Fx /home/user/documents/projekt.docs eller C:\\Users\\Username\\Documents\\projekt.docx.\nRelativ sti: En sti, der beskriver placeringen af en fil eller mappe i forhold til den nuværende mappe. Hvis vi er i mappen /home/user, kan vi nøjes med den relative sti documents/projekt.docx for at henvise til filen.\n\n\n1ls\n     ls -l\n     ls -a\n\n2cd\n     cd ..\n     cd ~\n     cd -\n\n3touch filnavn.type\n\n4mkdir ny-mappe\n\n5rm filnavn.type\n\n6rm -r ny-mappe\n\n\n1\n\nLister filer og mapper i den aktuelle mappe. ls -l lister filer og mapper med detaljer (fx rettigheder, størrelse). ls -a viser alle filer, inklusiv skjulte filer.\n\n2\n\nSkifter til en anden mappe. cd .. går én mappe op/tilbage (til forældermappen). cd ~ går til brugerens hjemmemappe. cd - skifter tilbage til den seneste mappe, du var i.\n\n3\n\nOpretter en ny, tom fil med angivet navn og type.\n\n4\n\nOpretter en ny mappe med det angivne navn.\n\n5\n\nSletter en fil med det angivne navn.\n\n6\n\nSletter en mappe og alt indholdet i den rekursivt.\n\n\n\n\n\nEr navigation med CML nødvendigt? Nej. Men det kan give et flow, hvis vi primært laver kodearbejde da terminalen kan tilgås “indeni” programmer som Rstudio eller vscode. Men uanset om man bruger mus eller tastetur til at navigere på sin computer, er det vigtigt at vide, hvordan filer er organiseret, hvis man har en computer-baseret stilling."
  },
  {
    "objectID": "workshop/decomposition.html",
    "href": "workshop/decomposition.html",
    "title": "",
    "section": "",
    "text": "Decomposition"
  },
  {
    "objectID": "workshop/nlp.html",
    "href": "workshop/nlp.html",
    "title": "",
    "section": "",
    "text": "Hvad er NLP?\nNatural Language Processing (NLP) er et centralt felt indenfor AI (kunstig intellegens). Grundlæggende handler NLP om hvordan en computer kan forstå og fortolke naturligt sprog, dvs. menneskeligt talt sprog. Gerne opgaven er at maskiner kan bearbejde dette sprog (og endda kunne producere det).\nI dag spiller NLP en stor rolle i vores hverdag, da det påvirker den måde vi interagerer med teknologi og gør denne interaktion meget mere effektiv. Vi kan kun forvente at dette samspil, takket være NLP, fortsat blive mere effektivt og af større betydning i fremtiden.\n\nI kender allerede til NLP\nNLP er allerede dybt integreret i mange af de værktøjer og teknologier vi anvender eller bliver eksponeret til dagligt:\n\nSøgemaskinger: Google (og konkurrenter) bruger NLP til at forstå de input og returnere det du faktisk efterspørger. Det er derfor søgemaskiner i dag kan “overkomme” stavefejl, synonymer, kontekst specikke forespørgsler, osv.\nSiri, Alexa, Google Assistant: De lytter til os hele tiden, hvis først vi tænder for dem …\nOversættelser (mellem menneskelige sprog): Services som Google Translate oversætter ikke bare ord-for-ord men forstår sig også på forskelle i syntaks, grammatik og (sproglige) kontekster og konventioner.\nChatbots …\nSpamfiltrering: Som vi kommer til at lære i dag.\n\n\n\nEt oversættelsesperspektiv\nComputeren forstår ikke sprog på samme måde som mennesker gør. De kan læse 1 og de kan læse 0; men de kan sætte disse tegn sammen i uendelige rækker af varierende kompleksitet. Dvs. mønstre af binære numeriske inputs. NLP handler om at bygge bro mellem den måde, mennesker kommunikerer på, og hvordan maskiner forstår data.\nEn IKKE-UDTØMMENDE liste af grundlæggende elementer i oversættelse af naturligt sprog til maskin-læsbart sprog:\n\nTokenization, som handler om at dele en tekst op i mindre dele, ofte ord eller sætninger. En sætning som “Jeg elsker data!” blive delt op i tre(fire) tokens: [“Jeg”, “elsker”, “data”, “!”].\nStemming og Lemmatization, som reducerer ord til deres grundform. Fx bliver “løbende” og “løber” reduceret til roden, “løb”.\nPart-of-Speech Tagging (POS Tagging), som identificerer ordklasser (som verber, substantiver osv.) for hvert ord i en sætning, hvilket gør det muligt at forstå ordenes funktion i sætningen.\nNamed Entity Recognition (NER), som identificerer navne på personer, steder eller organisationer i en tekst. For eksempel i sætningen “Aalborg Universitet er et universitet i Danmark” vil “Aalborg Universitet” blive genkendt som en organisation og “Danmark” som et land.\n\nDette oversættelsesperspektiv i en digital kontekst er centralt i dagens workshop."
  },
  {
    "objectID": "workshop/api.html",
    "href": "workshop/api.html",
    "title": "",
    "section": "",
    "text": "API’s"
  },
  {
    "objectID": "workshop/lda.html",
    "href": "workshop/lda.html",
    "title": "",
    "section": "",
    "text": "XXX\n\nRPythonsessionInfo\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\nimport matplotlib.pyplot as plt\n...\nplt.show()\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       lattice_0.22-5    png_0.1-8         digest_0.6.33    \n [5] grid_4.2.1        jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2      \n [9] cli_3.6.2         rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0\n[13] rmarkdown_2.28    tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42        \n[17] yaml_2.3.8        fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7  \n[21] knitr_1.45"
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshops and teaching",
    "section": "",
    "text": "Lorem ipsum odor amet, consectetuer adipiscing elit. Quam nullam pretium malesuada potenti commodo rutrum molestie tincidunt sodales. Risus nulla dui faucibus odio est phasellus tempus. Sollicitudin dapibus nunc ex congue nostra sapien velit. Praesent pellentesque vitae sociosqu orci magnis habitant maximus metus quisque. Velit tempus ad sodales hac; suspendisse suscipit.\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem. Himenaeos adipiscing ultrices ex; rutrum dignissim turpis. Libero sociosqu lacinia nibh; potenti elit viverra. Senectus fames fringilla praesent nascetur lacus lobortis dui tortor. Felis ridiculus lorem senectus convallis conubia nec phasellus nisi. Potenti sodales rhoncus et penatibus auctor morbi erat augue iaculis. Egestas dictumst phasellus class nec facilisis sapien lectus maximus iaculis. Quam nascetur fusce vivamus proin dolor; magna tempus curae."
  },
  {
    "objectID": "workshop.html#descriptive",
    "href": "workshop.html#descriptive",
    "title": "Workshops and teaching",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nLorem ipsum odor amet, consectetuer adipiscing elit. Quam nullam pretium malesuada potenti commodo rutrum molestie tincidunt sodales. Risus nulla dui faucibus odio est phasellus tempus. Sollicitudin dapibus nunc ex congue nostra sapien velit. Praesent pellentesque vitae sociosqu orci magnis habitant maximus metus quisque. Velit tempus ad sodales hac; suspendisse suscipit.\n\n\n R\n Python\n sessionInfo()\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n1+1\n\n2\n\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45"
  },
  {
    "objectID": "workshop.html#regression",
    "href": "workshop.html#regression",
    "title": "Workshops and teaching",
    "section": "Regression",
    "text": "Regression\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\nMazda RX4\n21.0\n6\n160\n\n\nMazda RX4 Wag\n21.0\n6\n160\n\n\nDatsun 710\n22.8\n4\n108\n\n\n\n\nRegression"
  },
  {
    "objectID": "workshop.html#segregation",
    "href": "workshop.html#segregation",
    "title": "Workshops and teaching",
    "section": "Segregation Indices",
    "text": "Segregation Indices\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem."
  },
  {
    "objectID": "workshop.html#decomposition",
    "href": "workshop.html#decomposition",
    "title": "Workshops and teaching",
    "section": "Decomposition",
    "text": "Decomposition\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nDecomposition"
  },
  {
    "objectID": "workshop.html#sna",
    "href": "workshop.html#sna",
    "title": "Workshops and teaching",
    "section": "SNA and VNA",
    "text": "SNA and VNA\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem."
  },
  {
    "objectID": "workshop.html#nlp",
    "href": "workshop.html#nlp",
    "title": "Workshops and teaching",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nHvad er NLP?\nNatural Language Processing (NLP) er et centralt felt indenfor AI (kunstig intellegens). Grundlæggende handler NLP om hvordan en computer kan forstå og fortolke naturligt sprog, dvs. menneskeligt talt sprog. Gerne opgaven er at maskiner kan bearbejde dette sprog (og endda kunne producere det).\nI dag spiller NLP en stor rolle i vores hverdag, da det påvirker den måde vi interagerer med teknologi og gør denne interaktion meget mere effektiv. Vi kan kun forvente at dette samspil, takket være NLP, fortsat blive mere effektivt og af større betydning i fremtiden.\nI kender allerede til NLP\nNLP er allerede dybt integreret i mange af de værktøjer og teknologier vi anvender eller bliver eksponeret til dagligt:\n\nSøgemaskinger: Google (og konkurrenter) bruger NLP til at forstå de input og returnere det du faktisk efterspørger. Det er derfor søgemaskiner i dag kan “overkomme” stavefejl, synonymer, kontekst specikke forespørgsler, osv.\nSiri, Alexa, Google Assistant: De lytter til os hele tiden, hvis først vi tænder for dem …\nOversættelser (mellem menneskelige sprog): Services som Google Translate oversætter ikke bare ord-for-ord men forstår sig også på forskelle i syntaks, grammatik og (sproglige) kontekster og konventioner.\nChatbots …\nSpamfiltrering: Som vi kommer til at lære i dag.\nEt oversættelsesperspektiv\nComputeren forstår ikke sprog på samme måde som mennesker gør. De kan læse 1 og de kan læse 0; men de kan sætte disse tegn sammen i uendelige rækker af varierende kompleksitet. Dvs. mønstre af binære numeriske inputs. NLP handler om at bygge bro mellem den måde, mennesker kommunikerer på, og hvordan maskiner forstår data.\nEn IKKE-UDTØMMENDE liste af grundlæggende elementer i oversættelse af naturligt sprog til maskin-læsbart sprog:\n\nTokenization, som handler om at dele en tekst op i mindre dele, ofte ord eller sætninger. En sætning som “Jeg elsker data!” blive delt op i tre(fire) tokens: [“Jeg”, “elsker”, “data”, “!”].\nStemming og Lemmatization, som reducerer ord til deres grundform. Fx bliver “løbende” og “løber” reduceret til roden, “løb”.\nPart-of-Speech Tagging (POS Tagging), som identificerer ordklasser (som verber, substantiver osv.) for hvert ord i en sætning, hvilket gør det muligt at forstå ordenes funktion i sætningen.\nNamed Entity Recognition (NER), som identificerer navne på personer, steder eller organisationer i en tekst. For eksempel i sætningen “Aalborg Universitet er et universitet i Danmark” vil “Aalborg Universitet” blive genkendt som en organisation og “Danmark” som et land.\n\nDette oversættelsesperspektiv i en digital kontekst er centralt i dagens workshop."
  },
  {
    "objectID": "workshop.html#nb",
    "href": "workshop.html#nb",
    "title": "Workshops and teaching",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nCase …\nSuperviseret Machine Learning (SML)\nI dag er fokus kun på superviseret ML, da vi kun har en enkelt workshop i dag og der vil være for mange statistiske forudsætninger til de to andre hovedtyper.\nSML fungerer ved, at vi giver modellen data, hvor vi kender det rigtige svar. Det kunne være, om en besked er spam eller ej, om et produkt er populær baseret på salgsdata, eller hvilken temperatur der vil være i morgen baseret på historiske målinger.\n\nI besked-eksemplet vil det altså sige at vi har et datasæt bestående af SMSer, hvor hver SMS i den data vi træner vores model på er kodet, dvs. tilskrevet et label, der indikerer om SMSen er spam (label=1) eller ikke-spam – “ham” – (label=0).\n\nModellen lærer sammenhænge mellem de inputdata (features, ord), som vi fodrer den med, og de kendte svar (labels, spam/ham). Når modellen er trænet, og den er vurderet til at være god nok, kan vi bruge den til at forudsige labels for nye data, hvor vi ikke kender svaret på forhånd.\nDet største problem i at arbejde med tekst i ML er at ML-algoritme ikke kan arbejde direkte med tekst. De kræver numeriske inputs for at kunne udføre matematiske operationer (se grundbog).\nVi er altså nødt til at konvertere tekst til en numerisk repræsentation. I denne kontekst kaldes denne proces for vectorisering.\nI denne proces transformerer og repræsenterer vi hvert tekstdokument (fx en SMS) som en række tal eller en vektor.\nKlassifikationsalgoritme\nNaive Bayes er en algoritme til at løse et konkret klassifikationsproblem relateret til naturligt sprog\nNaive Bayes er en probabilistisk klassifikationsmodel, baseret på Bayes’ teorem. Algoritmen fungerer ved at beregne sandsynligheden for, at en besked tilhører en bestemt klasse, givet dens indhold (dvs. de ord, der optræder i beskeden).\nModellen kaldes for “Naiv” grundet en central antagelse om at alle features (i vores tilfælde ord) er uafhængige af hinanden. Denne antagelse er ikke realistisk, da ord normalt ikke optræder helt uafhængigt af hinanden (ord i sætninger er ofte afhængige af hinanden). Det gør algoritmen enkel og hurtig, og den fungerer alligevel godt i praksis, som er blevet illustreret i tekniske detaljer mange steder.\nModellens formål er at lære forholdet mellem de inputdata, vi giver den (features), og de kendte labels, så den kan forudsige labels for nye, ukendte data.\nDet vil altså sige at vi har med et klassifikationsproblem at gøre. Modellen skal forudsige, hvilken kategori noget tilhører, og virke som et spam-filter, hvor vi klassificerer beskeder som enten “spam” eller “ikke-spam”, og i en praktisk applikation kan sende indkomne beskeder ind i forskellige mapper, som I kender fra jeres e-mail.\nBayes’ Teorem\nBayes’ Theorem handler om at beregne betingede sandsynligheder, der giver os en måde at opdatere vores viden baseret på nye data. Det har givet navn til en hel gren i statistikke, bayesisk statistik, som står i kontrast til frekvensstatistik (som er det i med al sandsynlighed kender fra gymnasiet og det i skal lære på 4. semester).\nBayes’ teorem er givet ved:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\nHvor,\n\n\nP(A∣B): Sandsynligheden for A, givet B. Dette kalder vi den posterior sandsynligheden.\n\nP(B∣A): Sandsynligheden for B, givet A. Dette kalder vi likelihood (betinget sandsynlighed).\n\nP(A): Sandsynligheden for A uden at tage hensyn til B. Dette kalder vi prior sandsynligheden.\n\nP(B): Sandsynligheden for B, uanset hvad A er.\n\nI kontekst og i en klassifikationssammenhæng er:\n\n\nA klassen (spam eller ham), og\n\nB er de observerede data (de ord, der optræder i beskeden).\n\nog det vi er interesserde i er at bestemme P(spam∣ord): sandsynligheden for, at en besked er spam, givet at visse ord optræder.\nRepitation af formål og hvad vi vil implementere i Python er:\n\nSandsynligheden for at en SMS er spam, baseret på fremkomsten/tilstedeværelsen af et givent ord, er proportionelt til sandsynligheden for at ordet fremkommer i spam-SMSer og den a priori sandsynlighed for at en tilfældig SMS er spam.\n\n\nP(\\text{spam}|ord)\\propto P(ord|\\text{spam}) P(\\text{spam})\n\n\nSpg.: Hvordan implimenterer vi denne model i Python på en måde, der kan “lære” maskinen at genkende spam-SMSer?\n\nAnvendelse af algoritmen til tekstklassificering\nVi bruger Naive Bayes til tekstklassifikation for at forudsige om en besked er spam eller ham baseret på sandsynligheden for de enkelte ord, der optræder i beskeden, tilhører en given klasse.\nDen generelle Naive Bayes-klassifikator for to klasser (spam eller ikke-spam) er formuleret som:\n \\hat{y} = \\underset{c}{\\operatorname{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i | c) \nHvor,\n\n\n\\hat{y} er den forudsagte klasse.\n\nc er en af klasserne (spam eller ikke-spam).\n\nP(c) er prior sandsynligheden for klassen c (sandsynligheden for, at en tilfældig besked er spam).\n\nP(x_{i}∣c) er sandsynligheden for ordet x_{i}, givet klassen c.\n\nn er antallet af ord i beskeden.\n\nMed formlen beregner vi sandsynligheden for, at en besked tilhører hver klasse (spam eller ikke-spam), og vælger den klasse, der har den højeste sandsynlighed: Vi vælger den klasse (c), hvor P(c|x) er størst, indikeret i formlen med \\underset{c}{\\operatorname{argmax}}\nFremgangsmåde:\n\nPrior sandsynlighed, P(c), beregnes ved at tælle, hvor mange af vores træningsbeskeder, der er spam i forhold til det samlede antal beskeder: P(spam)= \\frac{\\text{Antal spam-beskeder}}{\\text{Totalt antal beskeder}}\nLikelihood (betinget sandsynlighed), P(x_{i}∣c) beregnes som: P(x_{i}∣spam)= \\frac{\\text{Antal spam-beskeder, der indeholder } x_{i}}{\\text{Antal spam-beskeder totalt}}\n\nHvad vi er udregner, er hvor ofte hvert unikke ord i vores SAMLEDE TEKSTMATERIALE optræder i spam-beskeder (eller ikke-spam-beskeder), og produktet af sandsynlighederne for hvert enkelt ord i en given tekst, definere om teksten sandsynligvis er spam (eller ikke-spam)\nSelvom algoritmen er “naiv” og antagelsen om at alle ord er uafhængige, i praksis som udgangspunkt ikke holder, bestemmer vi stadig sandsynligheden for om en besked er spam eller ham som produktet af sandsynlighederne for de enkelte ord. Det er mange gange vist at denne “fejlantagelse” ikke er et problem i større mængder tekstdata.\n\nVi bruger træningsdata til at beregne P(spam) og P(ham)\n\nVi beregner sandsynlighederne for ordene “Congratulations”, “won”, “free”, osv. under begge klasser (spam og ham). Altså, hvad er sandsynligheden for at “free” (x_{free}) tilhører hhv. spam og ham klassen, givet fremkomsten af x_{free} i SMSer klassificeret som spam eller ham.\nVi multiplicerer sandsynlighederne for de enkelte ord – $x_{Congratulation} + x_{won} + x_{free} + $ – og vælger den klasse med den højeste sandsynlighed. Altså, hvert ord i en SMS har en sandsynlighed for at tilhører spam eller ham, og givet disse enkelte ord, hvor sandsynligt er det så for at SMS i sin helhed er spam eller ham.\n\nAltså, hvis ord som “won” og “free” ofte forekommer i spam-beskeder, vil Naive Bayes tildele beskeder med (store) fremkomster af disse ord en høj sandsynlighed for at være spam, og med denne sandsynlighedsargumentation klassificere SMSen som spam.\nEn lille, men central, sidebemærkning…\nDer kan opstå en problematisk udfordring, hvis et ord i udenfor vores træningsdata ikke fremgår i træningsdataen, da det vil “nulstille” den samlede sandsynlighed når vi multiplicerer.\nDette overkommes ved at inkludere en metode, der kaldes Laplace-smoothing, hvor alle betingede sandsynligheder tilføjes en lille konstant (værdi), således at ingen sandsynligheder er 0:\nP(x_{i}∣c)= \\frac{\\text{Totalt antal ord i klassen }+V}{\\text{Antal gange ordet optræder i klassen} +1}\nHvor, V er størrelsen af ordforrådet i vores corpus (antallet af unikke ord i træningsdataene). Med dette undgår vi nul-sandsynligheder.\nModel-træning\n\nI arbejdet med superviseret Machine Learning arbejder vi med vores data som opdelt i hhv. trænings- og testdata. Den data vi arbejder med, er et datasæt som vi har kvalitativt kodet med de korrekte labels ud fra vores forhåndsviden. Med denne opdeling er det muligt både at træne vores model og evaluere vores model, for at kunne vurdere hvordan modellen performer på nye, usete data.\n\nTræningsdata\nTræningsdataen er det datasæt, som vi træner vores model på. Datasættet indeholder både features (ord) og labels (korrekte kategorier). Det vil sige, vi ved altså hvad den rigtige kategori til vores tekster er, for at vores model at udregne det mønster, der kendetegner hver kategori.\nMed andre ord, når vi træner en Naive Bayes-model, “lærer” den at forstå sammenhængen mellem de input og de tilknyttede labels.\nEn klassisk opdeling er, at træningsdataen udgør 80% af den kvalitativt kodet data.\n\nTestdata\nTestdata udgør den anden del af den kvalitativt kodede data (her 20%). Testdataene bruges til at evaluere modelens præstation og generaliseringsevne og formålet med testdata er at give et mål for, hvordan modellen vil præstere på nye, usete data. Det vil altså sige at modellen ikke har “set” denne data (og er grunden til at vi skal have Laplace Smoothing…)\nEksempel …\nKlargøring af tekstdata\nDer er flere måde, hvorpå vi kan vektoriserer tekster, men centrale of typiske i denne form for analyse er:\n\nBag of Words (BoW)\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\nBoW\nBag of Words er den mest simpel metode til at transformere tekst til numerisk form. Det fungerer ved at tælle, hvor mange gange hvert ord forekommer i et dokument, uden at tage højde for ordets rækkefølge eller kontekst. Resultatet er en vektor, der repræsenterer frekvensen af hvert ord i dokumentet. Eksempel på BoW:\nTekst 1: Jeg elsker spam\nTekst 2: Jeg kan ikke fordrage spam\nFørst opretter vi et ordforråd (vocabulary) baseret på alle de unikke ord i vores dokumenter:\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\nvocab = ['Jeg', 'elsker', 'spam', 'kan', 'ikke', 'fordrage']\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nffffffffffff\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\ntekst1 = [1, 1, 1, 0, 0, 0]\ntekst2 = [1, 0, 1, 1, 1, 1]\ndf = pd.DataFrame([tekst1, tekst2], columns=vocab, index=['tekst1', 'tekst2'])\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nffffffffffff\nTF-IDF\nTF-IDF tager, i modsætning til BoW, højde for, hvor ofte et ord forekommer i en tekst, i forhold til hvor ofte det forekommer i hele datasættet. Dette hjælper med at nedvægte meget almindelige ord (såsom “is”, “am,”the”, osv.), som sandsynligvis ikke bidrager meget til meningen af dokumentet, og fremhæve ord, der er særligt vigtige for den specifikke besked (såsom “free”, “won”).\nTF-IDF for et ord x i et dokument d er givet ved:\n\\text{TF-IDF}(x,d)=\\text{TF}(x,d) \\times \\text{IDF}(x)\nHvor:\n\nTF (Term Frequency): Måler hvor ofte ordet x forekommer i dokumentet d.\nIDF (Inverse Document Frequency): log ⁡\\left( \\frac{N}{df(x)} \\right ), hvor N er det totale antal dokumenter, og df(x) er antallet af dokumenter, som indeholder x.\n\nHermed sikrer vi at vi ikke vægter almindelige ord for højt, men i stedet fokuserer på de vigtigere ord.\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\ncorpus = [\"Jeg elsker spam\",\"Jeg kan ikke fordrage spam\"]\n\ndf = pd.DataFrame({'dokument': corpus})\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nffffffffffff\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn TF for hvert ord i dokumentet:\n\n# Tokenisere dokumenter:\n# hvad kalder vi det når vi skriver .apply(lambda x: ...)?\n# og hvad sker der?\ndf['tokens'] = df['dokument'].apply(lambda x: x.split())\n\n# Beregn antallet af ord i hvert dokument\n# Hvad sker der her?\ndf['total_ord'] = df['tokens'].apply(len)\n\n# En liste af alle tokens:\n# 1. Vi looper først over hver sublist i df['tokens'], der er alle ord i en tekst. Dvs. vi looper over hver række i kolonnen 'tokens'.\n# 2. Når vi har en specifik sublist, \"liste_med_ord\", looper vi nu over hvert enkelt token (ord, x) i denne subliste.\n# 3. For hvert token i hver sublist, føjes dette token til den nye liste alle_tokens med .append().\nalle_tokens = []\nfor liste_med_ord in df['tokens']:\n    for x in liste_med_ord:\n        alle_tokens.append(x)\n\n# Find de unikke tokens:\n# \"set()\" funktion er kun at gemme unikke elementer/værdier\n# \"sorted()\" er med for at organisere vores tokens alfabetisk, men er som sådan ikke nødvendig. Prøv evt. uden.\nunikke_tokens = sorted(set(alle_tokens))\n\n# Udregn TF for hvert dokument for hvert ord\nfor ord in unikke_tokens:\n    df[ord] = df['tokens'].apply(lambda x: x.count(ord) / len(x))\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nfffffffffff\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn IDF for hvert ord i dokumentet:\n\nimport math # For at få log()-funktionen\n\n# Beregne IDF for hvert ord\n# 1. Definer funktion\ndef bestem_idf(ord, df):\n    # Antal dokumenter der indeholder ordet\n    # 2.: df['tokens'] er en kollonne i vores DataFrame (df)\n    # 3.: .apply(lambda x: ord in x) for hvert dokument (SMS),\n    #     repræsenteret som en liste af ord, tjekker vi om ordet er til stede\n    #     i dokumentet. Funktionen returnerer TRUE eller FALSE (ord in x: True or False?)\n    # 4.: TRUE og FALSE repræsenteres nummerisk som 1 og 0. Ved at summere alle 1ere og 0ere,\n    #     får vi antallet af dokumenter, der indeholder ord x.\n    doks_med_ord = df['tokens'].apply(lambda x: ord in x).sum()\n    # Beregn IDF\n    # 5.: len() giver en værdi for antallet af dokumenter (SMSer). Tælleren i formlen.\n    #     (1 + doks_med_ord) er nævneren i formlen\n    #     .log(...) tager logaritmen.\n    return math.log(len(df) / (1 + doks_med_ord))\n\n# Beregn IDF for hvert unikke ord\n    # 6. Dette kalder vi en \"dictionary comprehension\", fordi koden her går\n    #    gennem alle ord i unikke_tokens og for HVERT ORD i unikke_tokens\n    #    kaldes vores definerede funktion \"bestem_idf\" og tilknytter en IDF-score\n    #    til dette ord.\nidf_scores = {ord: bestem_idf(ord, df) for ord in unikke_tokens}\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nfffffffffff\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\n# Beregn TF-IDF for hvert ord i hvert dokument\n# 1.: df består af entelte ord (tokens) med en TF værdi (udregnet ovenfor),\n#     hvor unikke_tokens repræsenterer alle unikke ord, som vi looper henover.\n# 2.: Hvert ord har en tilknyttet IDF-værdi, som er udregnet med \"bestem_idf\",\n#     og gemt i \"idf_scores\".\n# Det som dette loop gør er at multiplicere hver enkelt ord TF med IDF, og får\n# dermed TF-IDF for HVERT ORD i vores samlede dokumentdata.\nfor ord in unikke_tokens:\n    df[ord] = df[ord] * idf_scores[ord]\n\n# Print beregnede scores:\n# Hvad sker der her, hvor jeg indenfor [[...]] også anvender en vektor med alle unikke ord?\nprint(df[['dokument'] + unikke_tokens])\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45       \n\n\n\n\n\nTL;DR\nOpsummeret,\n\nKoden gennemgår hvert unikt ord i dokumenterne.\nVi tæller, hvor mange dokumenter (SMSer) der indeholder det specifikke ord.\nVi beregner IDF for hvert ord baseret på, hvor mange dokumenter det optræder i.\nIDF-værdierne gemmes i en dictionary (idf_scores), hvor hvert ord har en tilknyttet IDF-værdi.\nFor hvert ord multiplicerer vi den tilhørende TF og IDF værdi for at få TF-IDF\nTræning\nTest\nEvaluering"
  },
  {
    "objectID": "workshop.html#lda",
    "href": "workshop.html#lda",
    "title": "Workshops and teaching",
    "section": "LDA",
    "text": "LDA\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\n\n\nHi! I’m a side note …………………………………. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, !\nXXX\n\n\nR\nPython\nsessionInfo\n\n\n\n\nx &lt;- seq(1, 10)\n...\nplot(x, y)\n\n\n\n\nimport matplotlib.pyplot as plt\n...\nplt.show()\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11       here_1.0.1        lattice_0.22-5    png_0.1-8        \n [5] withr_3.0.0       rprojroot_2.0.4   digest_0.6.33     grid_4.2.1       \n [9] jsonlite_1.8.8    evaluate_0.23     rlang_1.1.2       cli_3.6.2        \n[13] rstudioapi_0.15.0 Matrix_1.5-3      reticulate_1.34.0 rmarkdown_2.28   \n[17] tools_4.2.1       htmlwidgets_1.6.4 xfun_0.42         yaml_2.3.8       \n[21] fastmap_1.1.1     compiler_4.2.1    htmltools_0.5.7   knitr_1.45"
  },
  {
    "objectID": "workshop.html#python",
    "href": "workshop.html#python",
    "title": "Workshops and teaching",
    "section": "Python",
    "text": "Python\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem."
  },
  {
    "objectID": "workshop.html#r",
    "href": "workshop.html#r",
    "title": "Workshops and teaching",
    "section": "R",
    "text": "R\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem."
  },
  {
    "objectID": "workshop.html#quarto",
    "href": "workshop.html#quarto",
    "title": "Workshops and teaching",
    "section": "Quarto",
    "text": "Quarto\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem."
  },
  {
    "objectID": "workshop.html#cli",
    "href": "workshop.html#cli",
    "title": "Workshops and teaching",
    "section": "CommandLine and file organization [da]",
    "text": "CommandLine and file organization [da]\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nTerminalen: interaktion med computeren (og filsystemet)\n\nTerminalen er det, der giver os adgang til kommandolinjegrænsefladen (CLI). Selvom den har miste meget af sin position blandt den gennemsnitlige computer-bruger—grundet grafiske brugergrænseflader (GUI)—er den fortsat en meget effektiv måde at interagere med computeren. Særligt på Unix-systemer.\nShell\nNår vi anvender CLI, bruger vi en shell, der er et program til fortolkning af kommandoer. De to mest almindelige shell-programmer er:\n\n\nBash (Bourne Again Shell): Standard på mange Linux-distributioner og tidligere på macOS.\n\nZsh (Z Shell): Standard på macOS fra og med version 10.15 Catalina.\nFilorganisering\n\nReferer til hvordan vores filer (data) og mapper (directories) er struktureret og lagret på vores lagringsenhed (harddisk, SSD, ekstern enhed, …).\nDenne struktur bestemmer hvordan data hentes og gemmes, og gør det muligt for brugeren eller programmer at finde, tilgå og anvende filer.\nEn mappe (directory) er en container, som indeholder filer og andre mapper, og danner grundlaget for en hierakisk struktur (tree-/træstruktur). Opbygningen er med afsæt i en root-mappe (ikke den egentlige root-mappe, men brugerens hjemmemappe), som indeholder undermapper og filer. Herfra indeholder hver undermappe andre undermapper og filer, hvilket danner et træ af mapper og filer, hvis vi zoomer ud. Med andre ord, (træ-)hierakiet giver en logisk og navigérbar organisering på computeren.\n\nroot directory: I Unix-systemer (MacOS, Linux) betegnes den /. I Windows er der en root-mappe i hvert drev, betegnet med bogstavet for drevet, fx C:\\\nUndermapper: Mapper, der findes inde i andre mapper, fx /home/user/documents eller C:\\Users\\Username\\Documents.\nEt filsystem\n\n\n\nUNIX (MacOS, Linux)\n/\n├── bin                  # Vigtige eksekverbare systemfiler\n├── sbin                 # Systemadministrative eksekverbare filer\n├── etc                  # Systemkonfigurationsfiler\n├── home                 # Brugermapper (personlige filer)\n│   └── jeppe            # Brugeren \"jeppe\"'s hjemmemappe\n│       ├── Documents    # jeppes dokumenter\n│       ├── Downloads    # jeppes downloadede filer\n│       ├── Music        # jeppes musikfiler\n│       ├── Pictures     # jeppes billeder\n│       ├── Videos       # jeppes videofiler\n│       └── Projects     # Personlige kodeprojekter og scripts\n│           └── snake_game\n│               ├── main.py # Python-kode til et snake-spil\n│               └── assets  # Grafikfiler til spillet\n├── root                 # Superbrugerens hjemmemappe\n├── usr                  # Bruger- og systemprogrammer\n│   ├── bin              # Programmer installeret til brugere\n│   ├── lib              # Systemets biblioteker\n│   └── local            # Lokalt installerede programmer\n├── var                  # Variable data som logs og mails\n│   ├── log              # Systemets logfiler\n│   └── tmp              # Midlertidige filer\n├── tmp                  # Midlertidige filer (slettes ved genstart)\n├── dev                  # Systemets enheder som harddiske og terminaler\n├── mnt                  # Monteringspunkt for midlertidige enheder\n│   └── usb-drive        # En USB-nøgle monteret her\n└── media                # Monteringspunkt for eksterne enheder\n    └── jeppe-usb        # jeppes eksterne harddisk hvis monteret\n\n\n\nWindows\nC:\\\n├── Program Files            # Programmer installeret for alle brugere\n├── Program Files (x86)      # 32-bit versioner (på 64-bit systemer)\n├── Users                    # Brugermapper (til hver bruger på systemet)\n│   └── jeppe                # Brugeren \"jeppe\"'s hjemmemappe\n│       ├── Documents        # jeppes dokumenter\n│       ├── Downloads        # jeppes downloadede filer\n│       ├── Music            # jeppes musikfiler\n│       ├── Pictures         # jeppes billeder\n│       ├── Videos           # jeppes videofiler\n│       ├── Desktop          # Filer og genveje på jeppes skrivebord\n│       ├── AppData          # jeppes personlige app-data og indstillinger\n│       └── Projects         # Personlige kodeprojekter og scripts\n│           └── snake_game\n│               ├── main.py  # Python-kode til et snake-spil\n│               └── assets   # Grafikfiler til spillet\n├── Windows                  # Operativsystemets filer\n│   ├── System32             # Vigtige systemfiler \n│   └── Temp                 # Midlertidige filer, der bruges af systemet\n├── ProgramData              # Data, der deles af applikationer på systemet\n└── Temp                     # Midlertidige filer\n\n\n\n\nNavigation: Absolutte og relative stier\n\nAbsolut sti: En sti, der beskriver placeringen af en fil eller mappe i forhold til root-mappen. Fx /home/user/documents/projekt.docs eller C:\\Users\\Username\\Documents\\projekt.docx.\nRelativ sti: En sti, der beskriver placeringen af en fil eller mappe i forhold til den nuværende mappe. Hvis vi er i mappen /home/user, kan vi nøjes med den relative sti documents/projekt.docx for at henvise til filen.\n\n\n1ls\n     ls -l\n     ls -a\n\n2cd\n     cd ..\n     cd ~\n     cd -\n\n3touch filnavn.type\n\n4mkdir ny-mappe\n\n5rm filnavn.type\n\n6rm -r ny-mappe\n\n\n1\n\nLister filer og mapper i den aktuelle mappe. ls -l lister filer og mapper med detaljer (fx rettigheder, størrelse). ls -a viser alle filer, inklusiv skjulte filer.\n\n2\n\nSkifter til en anden mappe. cd .. går én mappe op/tilbage (til forældermappen). cd ~ går til brugerens hjemmemappe. cd - skifter tilbage til den seneste mappe, du var i.\n\n3\n\nOpretter en ny, tom fil med angivet navn og type.\n\n4\n\nOpretter en ny mappe med det angivne navn.\n\n5\n\nSletter en fil med det angivne navn.\n\n6\n\nSletter en mappe og alt indholdet i den rekursivt.\n\n\n\n\n\nEr navigation med CML nødvendigt? Nej. Men det kan give et flow, hvis vi primært laver kodearbejde da terminalen kan tilgås “indeni” programmer som Rstudio eller vscode. Men uanset om man bruger mus eller tastetur til at navigere på sin computer, er det vigtigt at vide, hvordan filer er organiseret, hvis man har en computer-baseret stilling."
  },
  {
    "objectID": "workshop.html#git",
    "href": "workshop.html#git",
    "title": "Workshops and teaching",
    "section": "Git [da]",
    "text": "Git [da]\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\n\n.git and versioncontrol\nFor installation, se …\nVersionkontrol er et system (software), der holder styr på ændringer af filer over tid, der gør det muligt at genskabe vores tidligere arbejde. Virker for (stort set) alle filer.\nPå større projekter—hvor flere er involveret—er det vigtigt at have kontrol over, hvem der foretager ændringer, hvilke ændringer der er blevet foretaget, og hvordan man kan rulle tilbage til tidligere versioner, hvis noget går galt.\nI et langsigtet perspektiv vil Git hjælpe dig med at holde et projekt organiseret, muliggøre (mere) effektivt samarbejde og sikre, at vi altid har en backup af dine fremskridt.\n\n\nGør dette …\n/projektarbejde\n└──/backup\n    ├── projekt_281024.docx\n    ├── projekt_311024.docx\n    ├── projekt_041224.docx\n    ├── projekt_final.docx\n    ├── projekt_final2.docx\n    ├── projekt_final3.docx\n    ├── projekt_final_final.docx\n    └── projekt_FINAL.docx\n\n… til dette\n/projektarbejde\n├── .git\n└── projekt.docx\n\n\nGit i praksis\nVersionsstyringsprocessen med afsæt i .git skelletet består af 3 stadier:\n\nWorking Directory: den mappe, hvor vi kørte git init. Alt der ændres her spores af Git, men det gemmes (committes) ikke automatisk . Arbejdsområdet er der hvor .git er gemt og indeholder vores faktiske filer og mapper, som vi ser og redigerer på din computer. Når vi redigerer en fil i vores projekt, bliver ændringen først gjort i arbejdsområdet. Filer, der arbejdes på, får tagget M (modified), som betyder at Git har registeret en ændring, men den er ikke blevet gemt i versionshistorikken endnu.\nStaging Area: De ændringer, som du ønsker registreret i næste commit bliver flyttet til et staging area med git add . (se ④ nedenfor). Det er ikke som sådan et “sted”, men et snarer et “tag” til de filer, som Git skal gemme. Ingen ændringer er blevet gemt endnu. Det tekniske navn er index, og Stating Area er ikke et “sted” på computeren men en fil i .git mappen, der noterer hvad der skal sendes til versionshistorikken i næste git commit (se ⑥ nedenfor) og er et mellemstadie mellem Working Directory og Repository. Se det som et kladdeområde, hvor du forbereder de ændringer, der skal indgå i en commit. Vi sender filer til Staging Area med: git add. Den primære funktion er at holde vores versionshistorik ren og logisk opdelt. Hvilket gør det lettere at spore ændringer og identificere bugs senere. For at se hvad der er modificeret og/eller staged bruger vi: git status (se &#9314 nedenfor).\nRepository: Når vi bruger kommandoen git commit -m \"besked\" gemmes alt staged data i vores Git-repository og alle ændringer siden sidste commit bliver en permanent del af projektets versionshistorik. Vores repository er commit-historikken, hvor hver commit repræsenterer en version af projektet på et bestemt tidspunkt. Når filer er committed er det sikkert gemt i vores lokale database. Vi sender filer til versionshistorikken med: git commit &lt;fil&gt; (se ⑥ nedenfor). Vi tilgår historikken med: git log (se ⑦ nedenfor).\nBranching\nHver commit repræsenterer et punkt i projektets branch, og du kan navigere frem og tilbage i projektets historie efter behov.\nEn branch i Git repræsenterer en uafhængig udviklingslinje. Vi kan lave ændringer i denne branch uden at påvirke andre branches. Vi kan droppe en branch, hvis ideer var dårlig, eller merge den med vores primære branch, hvis det virkede. (Teknisk relaterer alt dette sig til HEAD-pointeren).\n“This makes using Git a joy because we know we can experiment without the danger of severely screwing things up.” (REF)\nEt sikkert workflow\n\n\nIsolering: Hver branch er isoleret fra andre branches, hvilket betyder, at ændringer i én branch ikke påvirker arbejdet i andre branches.\n\nSamarbejde: Udviklere kan arbejde på separate branches uden at forstyrre hinandens arbejde. Git gør det muligt at flette branches sammen, når arbejdet er færdigt.\n\nEksperimentering: Branches gør det nemt at eksperimentere med nye ideer uden risiko. Hvis noget går galt, kan du altid slette branch’en og vende tilbage til en stabil version.\nTilgå versionshistorikken og genskab tidligere stadie\ngit log\ngit checkout &lt;commit-id&gt;\nKommandoer\n\n1git config\n2git init\n3git status\n4git add\n5git diff\n6git commit\n7git log\n8git clone\n9git push\n10git pull\n11git remote\n\n\n1\n\nIndstilling af konfigurationsindstillinger (fx brugernavn og e-mail).\n\n2\n\nInitialiserer et nyt Git-repository i den aktuelle mappe. I skal være opmærksom på hvilken mappe I befinder jer i, når i kører git init.\n\n3\n\nViser status for ændringer i arbejdsområdet (fx hvilke filer der er ændret og klar til staging).\n\n4\n\nTilføjer filer til staging-området, så de er klar til næste commit.\n\n5\n\nViser forskelle mellem ændringer i filer, enten fra arbejdsområdet eller staging-området.\n\n6\n\nGemmer de ændringer, der er i staging-området, som en ny version i repository.\n\n7\n\nViser en log over commits i repository, ofte med detaljer som forfatter, dato og commit-besked.\n\n8\n\nHenter et eksisterende repository fra en ekstern kilde (fx GitHub) og opretter en lokal kopi.\n\n9\n\nSender lokale commits til et eksternt repository.\n\n10\n\nHenter og integrerer ændringer fra et eksternt repository til den lokale kopi.\n\n11\n\nAdministrerer forbindelser til eksterne repositories.\n\n\n\n\nI ① … ② Kommandoen skaber en ny undermappe (.git) og er “skelettet” for vores repository. Denne mappe indeholder alle Git’s interne data, der bruges til at spore og administrere versionshistorikken for dit projekt. ③ … ④ … ⑤ … ⑥ … ⑦ … ⑧ … ⑨ … ⑩ … ⑪ …\nLokalt repository\nDet lokale repository, er det ligger på vores lokalecomputer (.git mappen).\nFjern repository\nGrundlæggende fungerer et fjernrepositoryet som et centralt lager på en server, som flere udviklere kan samarbejde om. Disser servere er typisk hostet på platforme som GitHub eller GitLab.\nEt fjernrepositoty kan klones (se ⑧ ovenfor) til vores lokale computer, således vi har en lokal kopi af projektet. Herefter kan vi pull’e og push’e ændringer:\n\n\nPull: Henter ændringer fra fjernrepository’et til dit lokale repository.\n\nPush: Skubber ændringer fra dit lokale repository til fjernrepository’et.\nDistribueret versionskontrol\nGit er et distribueret versionskontrolsystem, hvilket betyder, at hver udvikler har en fuld kopi af hele repositoryet (inklusive historik og branches) på deres egen computer.\nØvelse"
  },
  {
    "objectID": "workshop.html#plaintext",
    "href": "workshop.html#plaintext",
    "title": "Workshops and teaching",
    "section": "Plain text",
    "text": "Plain text\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nPlain text"
  },
  {
    "objectID": "workshop.html#api",
    "href": "workshop.html#api",
    "title": "Workshops and teaching",
    "section": "API’s",
    "text": "API’s\nFaucibus accumsan ipsum et tempor fringilla placerat nisl ultrices. Suscipit dignissim finibus platea efficitur inceptos consequat orci sem.\nAPI’s"
  },
  {
    "objectID": "projects/direk.html",
    "href": "projects/direk.html",
    "title": "",
    "section": "",
    "text": "DIREK\n\n2+2 \n\n[1] 4"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jeppe Fjeldgaard Qvist",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome",
    "section": "",
    "text": "About this site…\n\n1 + 1\n\n[1] 2"
  }
]